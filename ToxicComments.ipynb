{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-28 19:24:54,129:root:INFO   :           getLogger:: creating logger for main under MODEL\n"
     ]
    }
   ],
   "source": [
    "from config import Config\n",
    "from pprint import pprint, pformat\n",
    "from logger import model_logger\n",
    "log = model_logger.getLogger('main')\n",
    "log.setLevel(Config.Log.MODEL.level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-28 19:24:54,313:root:INFO   :           getLogger:: creating logger for main under TRAINER\n",
      "2018-01-28 19:24:54,548:root:INFO   :           getLogger:: creating logger for main under DATAFEED\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "from trainer import Trainer, Feeder\n",
    "from datafeed import DataFeed\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "train_dataset = csv.reader(open('dataset/train.csv'))\n",
    "test_dataset = csv.reader(open('dataset/test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Sample = namedtuple('Sample', ['id','comment_text',\n",
    "                               'toxic','severe_toxic','obscene',\n",
    "                               'threat','insult','identity_hate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 153164)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "train_datapoints = []\n",
    "for i in list(train_dataset)[1:]:\n",
    "    _id, c, t, st, o, t, ins, ih = i\n",
    "    t, st, o, t, ins, ih = (int(_) for _ in [t, st, o, t, ins, ih])\n",
    "    c = unicodedata.normalize('NFD', c).encode('ascii','ignore').decode()\n",
    "    train_datapoints.append(Sample(_id, c, t, st, o, t, ins, ih))\n",
    "\n",
    "train_datapoints = sorted(train_datapoints, key=lambda x: len(x.comment_text))\n",
    "\n",
    "test_datapoints = []\n",
    "for i in list(test_dataset)[1:]:\n",
    "    _id, c = i\n",
    "    c = unicodedata.normalize('NFD', c).encode('ascii','ignore').decode()\n",
    "    test_datapoints.append(Sample(_id, c, 0, 0, 0, 0, 0, 0))\n",
    "\n",
    "test_datapoints = sorted(test_datapoints, key=lambda x: len(x.comment_text))\n",
    "\n",
    "len(train_datapoints), len(test_datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_datapoints = train_datapoints[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sample(id='d2ef2d76fe9aa1db', comment_text='    .         ', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0),\n",
       " Sample(id='d802642c20e064a4', comment_text='::I was right.', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0),\n",
       " Sample(id='d916708f967a17b4', comment_text=':Never ending!', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0),\n",
       " Sample(id='db8ece11eb69265f', comment_text='It is in Tampa', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0),\n",
       " Sample(id='dde498ebdf5c6f5b', comment_text='::::I am sure.', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0),\n",
       " Sample(id='df46837faaadfef4', comment_text='              ', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0),\n",
       " Sample(id='df9e7b4cf096504a', comment_text=':::FYI. Cheers', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0),\n",
       " Sample(id='dffea66ea8edbbc2', comment_text='jelious bitch!', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0),\n",
       " Sample(id='e2efed170f8e58e4', comment_text='Done, I think.', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0),\n",
       " Sample(id='e3a8f04c114fabd7', comment_text=':Poop diggity.', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_datapoints[1000:1010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [01:26<00:00, 106.05it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "299976"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "datapoints = train_datapoints\n",
    "INPUT_VOCAB = [word for dp in tqdm(datapoints) for word in word_tokenize(dp.comment_text)]\n",
    "INPUT_VOCAB = ['PAD', 'UNK'] + list(set(INPUT_VOCAB))\n",
    "len(INPUT_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_VOCAB = ['toxic','severe_toxic','obscene', 'threat','insult','identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORD_INDEX = {w: i for i, w in enumerate(INPUT_VOCAB)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, Config, input_vocab_size, output_vocab_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.hidden_dim = Config.hidden_dim\n",
    "\n",
    "        self.embed = nn.Embedding(self.input_vocab_size, self.hidden_dim)\n",
    "        self.encode = nn.GRUCell(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "        self.classify = [nn.Linear(self.hidden_dim, 2)\n",
    "                         for i in range (self.output_vocab_size)]\n",
    "\n",
    "        self.log = model_logger.getLogger('model')\n",
    "        self.log.setLevel(logging.INFO)\n",
    "        if Config.cuda:\n",
    "            self.cuda()\n",
    "            [i.cuda() for i in self.classify]\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        ret = torch.zeros(batch_size, self.hidden_dim)\n",
    "        if Config().cuda: ret = ret.cuda()\n",
    "        return Variable(ret)\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        seq = Variable(torch.LongTensor(seq))\n",
    "        if Config().cuda: seq = seq.cuda()\n",
    "        batch_size = seq.size()[0]\n",
    "        self.log.debug('{} seq size: {}'.format(type(seq.data), seq.size()))\n",
    "        seq_emb = self.embed(seq).transpose(1,0)\n",
    "        output = self.init_hidden(batch_size)\n",
    "        for token_emb in seq_emb:\n",
    "            self.log.debug('token_emb := {}'.format(token_emb))\n",
    "            self.log.debug('output := {}'.format(output))\n",
    "            output = self.encode(token_emb, output)\n",
    "                    \n",
    "        self.log.debug('output := {}'.format(output))\n",
    "    \n",
    "        ret = torch.stack([F.softmax(classify(output), dim=-1) \n",
    "                           for classify in self.classify])\n",
    "        self.log.debug('ret := {}'.format(ret))\n",
    "\n",
    "        self.log.debug('ret size: {}'.format(ret.size()))\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def seq_maxlen(seqs):\n",
    "    return max([len(seq) for seq in seqs])\n",
    "\n",
    "PAD = WORD_INDEX[INPUT_VOCAB[0]]\n",
    "def pad_seq(seqs, maxlen=0, PAD=PAD):\n",
    "    if type(seqs[0]) == type([]):\n",
    "        maxlen = maxlen if maxlen else seq_maxlen(seqs)\n",
    "        def pad_seq_(seq):\n",
    "            return seq + [PAD]*(maxlen-len(seq))\n",
    "        seqs = [ pad_seq_(seq) for seq in seqs ]\n",
    "    return seqs\n",
    "\n",
    "def batchop(datapoints, *args, **kwargs):\n",
    "    seq   = pad_seq([ [WORD_INDEX[w] for w in word_tokenize(d.comment_text)]\n",
    "                     for d in datapoints])\n",
    "    target = [(d.toxic, d.severe_toxic, d.obscene, d.threat, d.insult, d.identity_hate)\n",
    "              for d in datapoints]\n",
    "    seq, target = np.array(seq), np.array(target)\n",
    "    return (seq, ), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(input, target, loss_function=nn.NLLLoss(), *args, **kwargs):\n",
    "    loss = 0\n",
    "    target = Variable(torch.LongTensor(target))\n",
    "    if Config().cuda: target = target.cuda()\n",
    "    input = input.transpose(1,0)\n",
    "    batch_size = input.size()[0]\n",
    "    for i, t in zip(input, target):\n",
    "        log.debug('i, o sizes: {} {}'.format(i, t))\n",
    "        loss += loss_function(i, t.squeeze()).mean()\n",
    "        log.debug('loss size: {}'.format(loss))\n",
    "\n",
    "    return loss/batch_size\n",
    "\n",
    "def accuracy(input, target, *args, **kwargs):\n",
    "    accuracy = 0\n",
    "    target = Variable(torch.LongTensor(target))\n",
    "    if Config().cuda: target = target.cuda()\n",
    "    input = input.transpose(1,0)\n",
    "    batch_size = input.size()[0]\n",
    "    class_size = input.size()[1]    \n",
    "    for i, t in zip(input, target):\n",
    "        correct = (i.max(dim=1)[1] == t).sum()\n",
    "        accuracy += correct/class_size\n",
    "        \n",
    "    return (accuracy/batch_size).data[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of datafeed failed: Traceback (most recent call last):\n",
      "  File \"/home/paarulakan/environments/python/pytorch-py35/lib/python3.5/site-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/paarulakan/projects/kaggle/toxic-comments/datafeed.py\", line 64\n",
      "    self.data[ n * self.batch_size   :   (n+1) * self.batch_size) ],\n",
      "                                                                ^\n",
      "SyntaxError: invalid syntax\n",
      "]\n",
      "2018-01-28 23:28:24,018:root:INFO   :           getLogger:: creating logger for main under TRAINER\n",
      "2018-01-28 23:28:27,663:root:INFO   :           getLogger:: creating logger for model under MODEL\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batchop_predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-1ca816149f5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                 feeder = Feeder(train_feed, test_feed))\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mpredict_feed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_datapoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchop_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepr_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepr_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict_feed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batchop_predict' is not defined"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "log.setLevel(logging.INFO)\n",
    "model =  Model(Config(), len(INPUT_VOCAB), len(OUTPUT_VOCAB))\n",
    "if Config().cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "split_index = int( len(train_datapoints) * 0.85 )\n",
    "train_feed = DataFeed(train_datapoints[:split_index], batchop=batchop, batch_size=128)\n",
    "test_feed = DataFeed(train_datapoints[split_index:], batchop=batchop, batch_size=120)\n",
    "\n",
    "trainer = Trainer(model=model, loss_function=loss, accuracy_function=accuracy, \n",
    "                checkpoint=1, epoch=100,\n",
    "                feeder = Feeder(train_feed, test_feed))\n",
    "\n",
    "predict_feed = DataFeed(train_datapoints[split_index:], batchop=batchop_predict, batch_size=12)\n",
    "predictor = Predictor(model=model, repr_function=repr_function, feed=predict_feed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-py35",
   "language": "python",
   "name": "pytorch-py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
