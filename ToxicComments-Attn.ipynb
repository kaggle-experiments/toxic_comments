{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-29 03:14:10,487:root:INFO   :           getLogger:: creating logger for main under MODEL\n"
     ]
    }
   ],
   "source": [
    "from config import Config\n",
    "from pprint import pprint, pformat\n",
    "from logger import model_logger\n",
    "log = model_logger.getLogger('main')\n",
    "log.setLevel(Config.Log.MODEL.level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-29 03:14:10,545:root:INFO   :           getLogger:: creating logger for main under TRAINER\n",
      "2018-01-29 03:14:10,792:root:INFO   :           getLogger:: creating logger for main under DATAFEED\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "from trainer import Trainer, Feeder, Predictor\n",
    "from datafeed import DataFeed\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "train_dataset = csv.reader(open('dataset/train.csv'))\n",
    "test_dataset = csv.reader(open('dataset/test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Sample = namedtuple('Sample', ['id','comment_text',\n",
    "                               'toxic','severe_toxic','obscene',\n",
    "                               'threat','insult','identity_hate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 153164)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "train_datapoints = []\n",
    "for i in list(train_dataset)[1:]:\n",
    "    _id, c, t, st, o, t, ins, ih = i\n",
    "    t, st, o, t, ins, ih = (int(_) for _ in [t, st, o, t, ins, ih])\n",
    "    c = unicodedata.normalize('NFD', c).encode('ascii','ignore').decode()\n",
    "    train_datapoints.append(Sample(_id, c, t, st, o, t, ins, ih))\n",
    "\n",
    "test_datapoints = []\n",
    "for i in list(test_dataset)[1:]:\n",
    "    _id, c = i\n",
    "    c = unicodedata.normalize('NFD', c).encode('ascii','ignore').decode()\n",
    "    test_datapoints.append(Sample(_id, c, 0, 0, 0, 0, 0, 0))\n",
    "\n",
    "len(train_datapoints), len(test_datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_datapoints = train_datapoints[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sample(id='01ac9982edae9977', comment_text='\" \\n\\n Dear ,Welcome to Wikipedia!Unfortunately, using your e-mail address as your username is not a good idea. Wikipedia content is extensively copied and the site itself is one of the most visited sites in the world. Any edit you make on Wikipedia will have your username attached to it, and using your email address will make you a tempting target for spammers. We recommend that you change your username at Wikipedia:Changing username in order to prevent abuse.If you need any help, simply contact me on my talk page, or go to Wikipedia:Help desk. Another option is to place  on your own talk page, and someone will come shortly to help. Remember to sign your posts on talk pages with four tildes (~~~~). Again, welcome! -  at \"', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0),\n",
       " Sample(id='01ac9ea1271d7409', comment_text='Asshole, your dirty MF people tries to steal our glory and heavy persian (tajik) history by claiming us Awghans (mother/donkeyfuckers)', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0),\n",
       " Sample(id='01ad4ccccec8c8ad', comment_text='projective special orthogonal group needs to be written', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0),\n",
       " Sample(id='01adb423b7547078', comment_text='\":::: @RQG: A point you\\'re still missing is that there is more than one actual mathematical definition.  Different characterizations of a concept can be equivalent to each other, and which one is taken to be \"\"the\"\" definition may be a matter of convenience varying with the context.  As I pointed out above, one of the characterizations (as \"\"(A,V)\"\") superficially makes it appear that an affine space has more structure than a vector space (since the vector space is only \"\"V\"\" rather than \"\"(A,V)\"\").  But I explained why that is a superficial appearance, and that particular way of defining the concept of affine space is not the only way and is not sacred.    \\n \"', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0),\n",
       " Sample(id='01adfd302b3c8467', comment_text='August 2013 (UTC) \\n ::*Done.   17:20, 18', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0),\n",
       " Sample(id='01ae25c502702f12', comment_text='== Yet Another Frivolous Lawsuit == \\n\\n So what if the commercials had some false advertising in it. Eating yogurt as part of a balanced diet is still good for you. And that good bacteria that is in the yogurt has been there since the creation of yogurt.  The person suing was probably some fat idiot who ate like six serving of yogurt in one sitting everyday.  Just wait until we get some serious tort reform in America. It will put an end to this nonsense!', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0),\n",
       " Sample(id='01ae71463be0b983', comment_text=\"==Nottingham Panthers GA== \\n Thanks for your help on the Nottingham Panthers article GA review. I've now done all that was requested that you believed neccessary when the article was put on hold.\", toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0),\n",
       " Sample(id='01ae75b51c6f104d', comment_text='\"Welcome! \\n\\n Hello, and welcome to Wikipedia. Thank you for your contributions. I hope you like the place and decide to stay. Here are a few good links for newcomers: \\n *The five pillars of Wikipedia \\n *How to edit a page \\n *Help pages \\n *Tutorial \\n *How to write a great article \\n *Manual of Style \\n I hope you enjoy editing here and being a Wikipedian! By the way, please be sure to sign your name on talk pages using four tildes (~~~~) to produce your name and the current date, or three tildes (~~~) for just your name. If you have any questions, you can post to the help desk or ask me on my talk page. Again, welcome!    \\n\\n Welcome to Wikipedia, the free encyclopedia! You don\\'t have to log in to read or edit articles on Wikipedia, but creating an account is quick, free and non-intrusive, requires no personal information, and gives you many benefits, including: \\n *The use of a username of your choice \\n *The ability to view all your contributions via a \"\"My contributions\"\" link  \\n *Your own user page  \\n *Your own talk page which, if you choose, also allows users to send you messages without knowing your e-mail address \\n *The use of your own personal watchlist to which you can add articles that interest you \\n *The ability to rename pages \\n *The ability to upload images  \\n *The ability to customize the appearance and behavior of the website \\n *The eligibility to become an administrator  \\n *The right to be heard in votes and elections \\n *Your IP address will no longer be visible to other users. \\n\\n We hope that you choose to become a Wikipedian and create an account.  We hope you enjoy your time here on Wikipedia as a Wikipedian! \\n\\n ==Australian Smelt== \\n\\n I did a little cleanup on the Australian Smelt article you wrote. Please write in complete sentances on wikipedia. please use proper stub tags on short articles  you can find the full list at Wikipedia:WikiProject Stub sorting/Stub types. Please try to come closer to our standard formatting. Thank you very much for contributing.   \"', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0),\n",
       " Sample(id='01aeddeab0ac923d', comment_text=\"Why would we dismiss all our usual, established reliable sources and scrabble around for others? As I and others have said, we don't need two ways to say the same thing, especially if one is in a foreign language.\", toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0),\n",
       " Sample(id='01aeded1d76ab01c', comment_text='\" \\n\\n == Articles for Creation == \\n If you need an article created, you can list it in Wikipedia:Articles for creation, but please don\\'t create a new article like \"\"Paleolithic Revolution\"\" unless you\\'re actually adding content to it. Welcome, by the way!  \"', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_datapoints[1000:1010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [01:26<00:00, 1850.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "299976"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "datapoints = train_datapoints\n",
    "INPUT_VOCAB = [word for dp in tqdm(datapoints) for word in word_tokenize(dp.comment_text)]\n",
    "INPUT_VOCAB = ['<<PAD>>', '<<UNK>>'] + list(set(INPUT_VOCAB))\n",
    "len(INPUT_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<<PAD>>',\n",
       " '<<UNK>>',\n",
       " 'neuropsychological',\n",
       " 'ASSWIPES',\n",
       " 'Falbastak',\n",
       " 'Undauntable/Fearless/Brave/Courage',\n",
       " 'Canadien',\n",
       " 'Hardcore',\n",
       " 'fajita',\n",
       " 'Sarvajna']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_VOCAB[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_VOCAB = ['toxic','severe_toxic','obscene', 'threat','insult','identity_hate']\n",
    "INPUT_VOCAB = list(set(INPUT_VOCAB + OUTPUT_VOCAB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORD_INDEX = {w: i for i, w in enumerate(INPUT_VOCAB)}\n",
    "OUTPUT_IDS = [WORD_INDEX[i] for i in OUTPUT_VOCAB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('Falbastak', 0),\n",
       "  ('neuropsychological', 1),\n",
       "  ('ASSWIPES', 2),\n",
       "  ('Undauntable/Fearless/Brave/Courage', 3),\n",
       "  ('Canadien', 4),\n",
       "  ('Hardcore', 5),\n",
       "  ('fajita', 6),\n",
       "  ('Sarvajna', 7),\n",
       "  ('0810939932', 8),\n",
       "  ('India/Requested', 9)],\n",
       " 68628,\n",
       " 'Falbastak')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(WORD_INDEX.items()), key=lambda x: x[1])[:10], WORD_INDEX['<<PAD>>'], INPUT_VOCAB[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "In DC?  I'm not sure - but then, this town tends to be a bit prudish.  He seems to have fallen a bit flat generally, but I don't know why - it's not like American audiences haven't seen him before.  I wasn't watching - Downton Abbey was on Masterpiece Theatre. Che dicono a Signa?Lo dicono a Signa. \"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "`` In DC ? I 'm not sure - but then , this town tends to be a bit prudish . He seems to have fallen a bit flat generally , but I do n't know why - it 's not like American audiences have n't seen him before . I was n't watching - Downton Abbey was on Masterpiece Theatre . Che dicono a Signa ? Lo dicono a Signa. ``\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "_i = train_datapoints[random.choice(range(len(train_datapoints)))]\n",
    "print(_i.comment_text)\n",
    "print(\"\"\"\n",
    "\n",
    "\n",
    "\"\"\")\n",
    "print(\n",
    "      ' '.join( [INPUT_VOCAB[i] for i in \n",
    "                 [WORD_INDEX[j] for j in word_tokenize(_i.comment_text)]]\n",
    "              )\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def seq_maxlen(seqs):\n",
    "    return max([len(seq) for seq in seqs])\n",
    "\n",
    "PAD = WORD_INDEX[INPUT_VOCAB[0]]\n",
    "print(PAD)\n",
    "def pad_seq(seqs, maxlen=0, PAD=PAD):\n",
    "    if type(seqs[0]) == type([]):\n",
    "        maxlen = maxlen if maxlen else seq_maxlen(seqs)\n",
    "        def pad_seq_(seq):\n",
    "            return seq + [PAD]*(maxlen-len(seq))\n",
    "        seqs = [ pad_seq_(seq) for seq in seqs ]\n",
    "    return seqs\n",
    "\n",
    "def batchop(datapoints, *args, **kwargs):\n",
    "    indices = [d.id for d in datapoints]\n",
    "    seq   = pad_seq([ [WORD_INDEX[w] for w in word_tokenize(d.comment_text)]\n",
    "                     for d in datapoints])\n",
    "    target = [(d.toxic, d.severe_toxic, d.obscene, d.threat, d.insult, d.identity_hate)\n",
    "              for d in datapoints]\n",
    "    seq, target = np.array(seq), np.array(target)\n",
    "    return indices, (seq, ), (target,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(output, target, loss_function=nn.NLLLoss(), *args, **kwargs):\n",
    "    loss = 0\n",
    "    target = Variable(torch.LongTensor(target[0]))\n",
    "    if Config().cuda: target = target.cuda()\n",
    "    output = output.transpose(1,0)\n",
    "    batch_size = output.size()[0]\n",
    "    for i, t in zip(output, target):\n",
    "        log.debug('i, o sizes: {} {}'.format(i, t))\n",
    "        loss += loss_function(i, t.squeeze()).mean()\n",
    "        log.debug('loss size: {}'.format(loss))\n",
    "\n",
    "    return loss/batch_size\n",
    "\n",
    "def accuracy(output, target, *args, **kwargs):\n",
    "    accuracy = 0\n",
    "    target = Variable(torch.LongTensor(target[0]))\n",
    "    if Config().cuda: target = target.cuda()\n",
    "    output = output.transpose(1,0)\n",
    "    batch_size = output.size()[0]\n",
    "    class_size = output.size()[1]    \n",
    "    \n",
    "    for i, t in zip(output, target):\n",
    "        correct = (i.max(dim=1)[1] == t).sum()\n",
    "        accuracy += correct/class_size\n",
    "        \n",
    "    return (accuracy/batch_size).data[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "def repr_function(output, feed, batch_index):\n",
    "    results = []\n",
    "    output = output.transpose(1,0)\n",
    "    indices, (seq,), (classes,) = feed.nth_batch(batch_index)\n",
    "    print(output.size(), len(indices), len(seq), len(classes))\n",
    "    for i, o, s, c in zip(indices, output, seq, classes):\n",
    "        orig_s = feed.data_dict[i].comment_text\n",
    "        s = [INPUT_VOCAB[i] for i in s]\n",
    "        s = ' '.join(s)\n",
    "        results.append([orig_s, s] + list(c))\n",
    "        o = o.max(dim=1)[1]\n",
    "        results.append([' ', '  '] + o.data.cpu().numpy().tolist())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_train_datapoints = train_datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, Config, input_vocab_size, output_vocab_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.hidden_dim = Config.hidden_dim\n",
    "\n",
    "        self.embed = nn.Embedding(self.input_vocab_size, self.hidden_dim)\n",
    "        self.encode = nn.GRUCell(self.hidden_dim, self.hidden_dim)\n",
    "        self.attend = nn.Parameter(torch.FloatTensor(self.hidden_dim, self.hidden_dim))\n",
    "\n",
    "        self.classify = nn.Linear(self.hidden_dim, 2)\n",
    "        self.log = model_logger.getLogger('model')\n",
    "        self.size_log = self.log.getLogger('size')\n",
    "        self.log.setLevel(logging.INFO)\n",
    "        self.size_log.setLevel(logging.DEBUG)\n",
    "        if Config.cuda:\n",
    "            self.cuda()\n",
    "            \n",
    "    def logsize(self, tensor, name=''):\n",
    "        self.size_log.debug('{} <- {}'.format(tensor.size(), name))\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        ret = torch.zeros(batch_size, self.hidden_dim)\n",
    "        if Config().cuda: ret = ret.cuda()\n",
    "        return Variable(ret)\n",
    "    \n",
    "    def forward(self, seq, classes=OUTPUT_IDS):\n",
    "        seq = Variable(torch.LongTensor(seq))\n",
    "        classes = Variable(torch.LongTensor(classes))\n",
    "        if Config().cuda: \n",
    "            seq = seq.cuda()\n",
    "            classes = classes.cuda()\n",
    "            \n",
    "        batch_size = seq.size()[0]\n",
    "        self.log.debug('{} seq size: {}'.format(type(seq.data), seq.size()))\n",
    "        seq_emb = self.embed(seq).transpose(1,0)                  ;self.logsize(seq_emb, 'seq_emb')\n",
    "        seq_repr = []\n",
    "        output = self.init_hidden(batch_size)                     ;self.logsize(output, 'output')\n",
    "        for token_emb in seq_emb:\n",
    "            self.log.debug('token_emb := {}'.format(token_emb))\n",
    "            self.log.debug('output := {}'.format(output))\n",
    "            output = self.encode(token_emb, output)               ;self.logsize(output, 'output')\n",
    "            seq_repr.append(output)\n",
    "\n",
    "        seq_repr = torch.stack(seq_repr).transpose(1,0)           ;self.logsize(seq_repr, 'seq_repr')\n",
    "        outputs = []\n",
    "        attend = self.attend\n",
    "        self.logsize(attend, 'attend')\n",
    "        for class_ in classes:\n",
    "            class_emb = self.embed(class_)                  ;self.logsize(class_emb, 'class_emb')\n",
    "            attn = torch.mm(class_emb, attend).unsqueeze(0) ;self.logsize(attn, 'attn')\n",
    "            attn = attn.expand_as(seq_repr) ;self.logsize(attn, 'attn')\n",
    "            attended_outputs = torch.bmm(attn, seq_repr)                \n",
    "        \n",
    "            self.logsize(attended_outputs, 'attended_outputs')\n",
    "            output = attended_output.expand_as(seq_repr) * seq_repr\n",
    "            self.logsize(output, 'output')\n",
    "\n",
    "            output = output.sum(0).squeeze()                ;self.logsize(output, 'output')\n",
    "            output = self.classify(output)                  ;self.logsize(output, 'output')\n",
    "            output = F.softmax(output)\n",
    "            outputs.append(output)\n",
    "            \n",
    "        ret = torch.stack(outputs)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-29 03:46:37,540:root:INFO   :           getLogger:: creating logger for model under MODEL\n",
      "2018-01-29 03:46:37,541:root:INFO   :           getLogger:: creating logger for size under MODEL.model\n",
      "2018-01-29 03:46:37,739:MODEL.model.size:DEBUG  :             logsize:: torch.Size([97, 12, 200]) <- seq_emb\n",
      "2018-01-29 03:46:37,740:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,743:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,745:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,747:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,749:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,751:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,753:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,755:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,757:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,759:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,761:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,763:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,765:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,766:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,768:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,770:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,772:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,790:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,792:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,794:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,796:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,798:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,800:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,802:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,804:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,808:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,811:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,814:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,817:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,819:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,822:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,823:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,825:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,827:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,829:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,831:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,833:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,835:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,836:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,838:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,840:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,842:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,845:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,847:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,849:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,851:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,853:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,855:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,857:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,860:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,862:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,864:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,866:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,868:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,871:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,873:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,876:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,878:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,880:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,883:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,885:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,887:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,890:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,892:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,894:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,897:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,899:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,901:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,903:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,906:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,908:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,911:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,913:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,915:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,917:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,919:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-29 03:46:37,921:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,923:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,925:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,927:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,929:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,931:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,933:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,935:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,937:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,939:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,942:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,944:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,946:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,952:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,955:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,959:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,961:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,964:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,967:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,969:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,971:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,974:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 200]) <- output\n",
      "2018-01-29 03:46:37,975:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 97, 200]) <- seq_repr\n",
      "2018-01-29 03:46:37,976:MODEL.model.size:DEBUG  :             logsize:: torch.Size([200, 200]) <- attend\n",
      "2018-01-29 03:46:37,978:MODEL.model.size:DEBUG  :             logsize:: torch.Size([1, 200]) <- class_emb\n",
      "2018-01-29 03:46:37,979:MODEL.model.size:DEBUG  :             logsize:: torch.Size([1, 1, 200]) <- attn\n",
      "2018-01-29 03:46:37,980:MODEL.model.size:DEBUG  :             logsize:: torch.Size([12, 97, 200]) <- attn\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 6: wrong matrix size at /pytorch/torch/lib/THC/generic/THCTensorMathBlas.cu:453",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-7cef83d7722b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-7cef83d7722b>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(epochs, checkpoint)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepr_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepr_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_feed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_feed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/paarulakan/projects/kaggle/toxic-comments/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, batch_index)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbatch_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnth_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mListTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepr_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/paarulakan/projects/kaggle/toxic-comments/trainer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/paarulakan/environments/python/pytorch-py35/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-1024c75dfc4e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seq, classes)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m;\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'attn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_repr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m;\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'attn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mattended_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_repr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattended_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'attended_outputs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 6: wrong matrix size at /pytorch/torch/lib/THC/generic/THCTensorMathBlas.cu:453"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def  experiment(epochs=10, checkpoint=1):\n",
    "    model =  Model(Config(), len(INPUT_VOCAB), len(OUTPUT_VOCAB))\n",
    "    if Config().cuda:  model = model.cuda()\n",
    "        \n",
    "    split_index = int( len(train_datapoints) * 0.85 )\n",
    "    train_feed = DataFeed(train_datapoints[:split_index], batchop=batchop, batch_size=12)\n",
    "    test_feed = DataFeed(train_datapoints[split_index:], batchop=batchop, batch_size=12)\n",
    "\n",
    "    trainer = Trainer(model=model, loss_function=loss, accuracy_function=accuracy, \n",
    "                    checkpoint=checkpoint, epochs=epochs,\n",
    "                    feeder = Feeder(train_feed, test_feed))\n",
    "\n",
    "    predictor = Predictor(model=model, repr_function=repr_function, feed=test_feed)\n",
    "    output, results = predictor.predict(random.choice(range(test_feed.num_batch)))\n",
    "\n",
    "    for e in range(10000):\n",
    "        output, results = predictor.predict(random.choice(range(test_feed.num_batch)))\n",
    "        display(HTML(results._repr_html_()))\n",
    "        trainer.train()\n",
    "        \n",
    "experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_feed = DataFeed(train_datapoints[:100], batchop=batchop, batch_size=1)\n",
    "indices, (seq,), (target,) = dummy_feed.nth_batch(random.choice(range(dummy_feed.num_batch)))\n",
    "print(dummy_feed.data_dict[indices[0]])\n",
    "print([INPUT_VOCAB[i] for i in seq[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-py35",
   "language": "python",
   "name": "pytorch-py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
